{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b23ffc-22d8-4414-b2e4-66d45a03523d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using LLM-powered retreival and reranking - (Claude LLM + Bedrock Titan embedding)\n",
    "\n",
    "### Context\n",
    "\n",
    "Utilizing LLM-driven retrieval has the potential to yield more pertinent documents compared to retrieval based on embeddings. However, this advantage comes at the expense of increased latency and expenses. We will demonstrate that employing embedding-based retrieval initially, followed by a secondary retrieval stage for reevaluation, can offer a balanced solution.\n",
    "\n",
    "A recent surge in applications involving \"Develop a chatbot using your data\" has emerged in the past several months. This trend has been facilitated by frameworks such as LlamaIndex and LangChain. Many of these applications rely on a standard approach known as retrieval augmented generation (RAG):\n",
    "\n",
    "1) A vector store is employed to store unstructured documents (knowledge corpus).\n",
    "2) When presented with a query, a retrieval model is utilized to fetch relevant documents from the corpus, followed by a synthesis model that generates a response.\n",
    "3) The retrieval model retrieves the top-k documents based on the similarity of their embeddings to the query. It's important to note that the concept of top-k embedding-based semantic search has existed for over a decade and doesn't involve the use of LLM.\n",
    "\n",
    "The utilization of embedding-based retrieval offers numerous advantages:\n",
    "\n",
    "* Dot product calculations are swift and don't necessitate model invocations during query processing.\n",
    "* Although not flawless, embeddings can effectively capture the semantics of documents and queries. There's a subset of queries for which embedding-based retrieval yields highly relevant outcomes.\n",
    "\n",
    "However, embedding-based retrieval can exhibit imprecision and return irrelevant context for the query due to various factors. This subsequently diminishes the quality of the overall RAG system, irrespective of the LLM's quality.\n",
    "\n",
    "Addressing this challenge is not novel; existing information retrieval and recommendation systems have adopted a two-stage approach. The initial stage employs embedding-based retrieval with a higher top-k value to maximize recall while accepting a lower precision. Subsequently, the second stage utilizes a somewhat more computationally intensive process characterized by higher precision and lower recall (such as BM25) to \"rerank\" the initially retrieved candidates.\n",
    "\n",
    "Delving into the shortcomings of embedding-based retrieval would require an entire series of blog posts. This current post serves as an initial exploration of an alternative retrieval technique and its potential to enhance embedding-based retrieval methodologies.\n",
    " \n",
    "![LLM retrival works](./images/arch.png)\n",
    "\n",
    "### LLM Retrieval and Reranking\n",
    "\n",
    "LLM Retrieval and reranking strategy employs the LLM to determine the document(s) or sections of text that align with the provided query. The input prompt comprises a collection of potential documents, and the LLM is entrusted with choosing the pertinent group of documents while also assigning a score to gauge their relevance using an internal measurement.\n",
    "\n",
    "\n",
    "In this notebook we explain how to approach the retriever pattern of LLM-powered retrieval and reranking using Amazon Bedrock LLM and LlamaIndex\n",
    "\n",
    "#### LlamaIndex\n",
    "LlamaIndex is a data framework for your LLM application. It provides the following tools:\n",
    "\n",
    "* Offers data connectors to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "* Provides ways to structure your data (indices, graphs) so that this data can be easily used with LLMs.\n",
    "* Provides an advanced retrieval/query interface over your data: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "* Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, anything else).\n",
    "* LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "### LLM Used:\n",
    "We will be leveraging Bedrock - Anthropic Claude LLM and Bedrock Embedding (Titan model) for demonstration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320367c-42b6-46e3-b367-de7327d6e665",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will first install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d72382-652c-4108-bef4-b3c86fb76c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.305 --force-reinstall --quiet\n",
    "%pip install pypdf==3.15.2 --force-reinstall --quiet\n",
    "%pip install llama-index==0.9.9 --force-reinstall --quiet\n",
    "%pip install sentence_transformers==2.2.2 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb9b17-92c3-4e58-a12a-b35e629595de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pydantic==1.10.13 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1c19e-e102-462e-bbac-63fac3978940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sqlalchemy==2.0.21 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab68a3ee-f4c1-4830-856a-9c37f7a42364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c16404b-db81-4ee6-9c0f-cc4cdf3cb147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    get_response_synthesizer,\n",
    "    set_global_service_context,\n",
    "    StorageContext,\n",
    "    ListIndex\n",
    ")\n",
    "from llama_index.indices.postprocessor import LLMRerank\n",
    "from llama_index.llms import Bedrock\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8809c-b174-4970-8758-f867cf968d81",
   "metadata": {},
   "source": [
    "### Setup langchain and llama index\n",
    "\n",
    "In this step we will be creating of instance for LLM and embedding models. We will be using Claude and Titan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9222da53-3112-47ad-950d-832e020098ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Un comment the following lines to run from your local environment outside of the AWS account with Bedrock access\n",
    "\n",
    "#import os\n",
    "#os.environ['BEDROCK_ASSUME_ROLE'] = '<YOUR_VALUES>'\n",
    "#os.environ['AWS_PROFILE'] = 'bedrock-user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83932140-8e5b-457d-84ff-9cc3040c0576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/pavankrn/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/pavankrn/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import sagemaker\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "if not region:\n",
    "    session = sagemaker.session.Session()\n",
    "    region = session._region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755c31f6-3bea-4afd-9025-c547a5439bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0514e54-36cd-4faf-b824-4cce34177bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings import BedrockEmbedding\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\":0,\n",
    "    \"top_k\":10, \n",
    "    \"max_tokens_to_sample\":512\n",
    "}\n",
    "\n",
    "llm = Bedrock(\n",
    "                model=\"anthropic.claude-v2:1\",\n",
    "                context_size=512,\n",
    "                aws_region_name=region,\n",
    "                additional_kwargs=model_kwargs_claude\n",
    "             )\n",
    "\n",
    "embed_model = BedrockEmbedding().from_credentials(\n",
    "    aws_profile=None,model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3246b062-ef7b-4450-8ac5-db5713cefa0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_overlap = 0\n",
    "chunk_size = 512\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap,)\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8a8a2-0e95-4c56-bf0a-c558009d38d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3491600b-32fa-4559-8d25-c7d07a57b84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2022, source=filenames[0]),\n",
    "    dict(year=2021, source=filenames[1]),\n",
    "    dict(year=2020, source=filenames[2]),\n",
    "    dict(year=2019, source=filenames[3])]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa549fa-41ff-4712-91f1-c9c9cd8e03b6",
   "metadata": {},
   "source": [
    "As part of Amazon's culture, the CEO always includes a copy of the 1997 Letter to Shareholders with every new release. This will cause repetition, take longer to generate embeddings, and may skew your results. In the next section you will take the downloaded data, trim the 1997 letter (last 3 pages) and overwrite them as processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a0b300-0c6c-43be-a2ea-ae16fa0912f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "local_pdfs = glob.glob(data_root + '*.pdf')\n",
    "\n",
    "for local_pdf in local_pdfs:\n",
    "    pdf_reader = PdfReader(local_pdf)\n",
    "    pdf_writer = PdfWriter()\n",
    "    for pagenum in range(len(pdf_reader.pages)-3):\n",
    "        page = pdf_reader.pages[pagenum]\n",
    "        pdf_writer.add_page(page)\n",
    "\n",
    "    with open(local_pdf, 'wb') as new_file:\n",
    "        new_file.seek(0)\n",
    "        pdf_writer.write(new_file)\n",
    "        new_file.truncate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9eb1db-5209-493c-8d6b-25bc700fc96e",
   "metadata": {},
   "source": [
    "Now that you have clean PDFs to work with, you will enrich your documents with metadata, then use a process called \"chunking\" to break up a larger document into small pieces. These small pieces will allow you to generate embeddings without surpassing the input limit of the embedding model.\n",
    "\n",
    "In this example you will break the document into 1000 character chunks, with a 100 character overlap. This will allow your embeddings to maintain some of its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4429b839-e244-42b8-9b24-5368c1bb0a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for filename in filenames:\n",
    "    doc = SimpleDirectoryReader(input_files=[f\"data/{filename}\"]).load_data()\n",
    "    doc[0].doc_id = filename.replace(\".pdf\", \"\")\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca7948-128b-4a86-a32a-33dbaf07ca4d",
   "metadata": {},
   "source": [
    "### Build Document Summary Index\n",
    "\n",
    "We show two ways of building the index:\n",
    "- default mode of building the document summary index\n",
    "- customizing the summary query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8555b6d-5289-4803-bced-6b2f85dea3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(docs,\n",
    "    service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ba68d6-3a91-4b31-a8b9-b7cc6d33b5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = service_context.node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a413a570-686c-4865-a424-7f69b822b640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb7247-7ecc-411a-b3db-4161ff36b975",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f234932-6362-42a9-9d21-056b89aac40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "#https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html#qa-with-context\n",
    "\n",
    "CLAUDE_CHOICE_SELECT_PROMPT_TMPL = (\n",
    "    \"\"\"\n",
    "    \n",
    "    Human: A list of documents is shown below. Each document has a number next to it along  with a summary of the document. A question is also provided. \n",
    "    Respond with the respective document number. You should consult to answer the question, in order of relevance, as well as the relevance score. \n",
    "    The relevance score is an integer number from 1-10 based on how relevant you think the document is to the question.\\n\n",
    "    \n",
    "    Example format: \n",
    "    Document 1:\\n<summary of document 1>\n",
    "    Document 2:\\n<summary of document 2>\n",
    "    ...\\n\\n\n",
    "    Document 10:\\n<summary of document 10>\n",
    "    \n",
    "    Question: <question>\n",
    "    Answer:\n",
    "    Doc: 9, Relevance: 7\n",
    "    Doc: 3, Relevance: 4\n",
    "    Doc: 7, Relevance: 3\n",
    "\n",
    "    Let's try this now: \n",
    "    {context_str}\n",
    "\n",
    "    Question: {query_str}\n",
    "    \n",
    "    Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "claude_choice_select_prompt = Prompt(\n",
    "    CLAUDE_CHOICE_SELECT_PROMPT_TMPL, prompt_type=PromptType.CHOICE_SELECT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805847d9-a2c1-4930-98a9-98126e730000",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace86835-832e-4964-a025-428891aa2c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "    # configure retriever\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=vector_top_k,\n",
    "        choice_select_prompt=claude_choice_select_prompt\n",
    "\n",
    "    )\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if with_reranker:\n",
    "        # configure reranker\n",
    "        reranker = LLMRerank(\n",
    "            choice_batch_size=5, \n",
    "            top_n=reranker_top_n, \n",
    "            service_context=service_context, \n",
    "            choice_select_prompt=claude_choice_select_prompt\n",
    "\n",
    "        )\n",
    "        retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node.node.get_text()}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492945-9ac6-44d9-9588-d6f97a460b0c",
   "metadata": {},
   "source": [
    "Now, we will showcase how to do a two-stage pass for retrieval. Use embedding-based retrieval with a high top-k value in order to maximize recall and get a large set of candidate items. Then, use LLM-based retrieval to dynamically select the nodes that are actually relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20993200-b725-403e-8a79-e2571dab2ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes1 = get_retrieved_nodes(\n",
    "    \"How has AWS evolved?\", \n",
    "    vector_top_k=3, \n",
    "    with_reranker=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c65220b0-ce3d-4bfb-a409-35d7cc035a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dd3bc25-6ef1-46b7-869e-247c83af9d4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552629</td>\n",
       "      <td>Everybody agreed that having a persistent block store was important to a complete compute service;<br>however, to have one ready would take an extra year. The question became could we offer customers auseful service where they could get meaningful value before we had all the features we thought they wanted?We decided that the initial launch of EC2 could be feature-poor if we also organized ourselves to listen tocustomers and iterate quickly. This approach works well if you indeed iterate quickly; but, is disastrous if youcan’t. We launched EC2 in 2006 with one instance size, in one data center, in one region of the world, withLinux operating system instances only (no Windows), without monitoring, load balancing, auto-scaling, oryes, persistent storage. EC2 was an initial success, but nowhere near the multi-billion-dollar service it’sbecome until we added the missing capabilities listed above, and then some.<br>In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated<br>commodity. But, there’s a lot more to compute than just a server. Customers want various flavors of compute(e.g. server configurations optimized for storage, memory, high-performance compute, graphics rendering,machine learning), multiple form factors (e.g. fixed instance sizes, portable containers, serverless functions),various sizes and optimizations of persistent storage, and a slew of networking capabilities. Then, there’sthe CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors.We have important partnerships with these companies, but realized that if we wanted to push price andperformance further (as customers requested), we’d have to develop our own chips, too. Our first generalizedchip was Graviton, which we announced in 2018. This helped a subset of customer workloads run morecost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and<br>innovating on a new chip, that we had something remarkable with our Graviton2 chip, which provides up to40% better price-performance than the comparable latest generation x86 processors. Think about howmuch of an impact 40% improvement on compute is. Compute is used for every bit of technology.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.545199</td>\n",
       "      <td>Then, there’sthe CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors.We have important partnerships with these companies, but realized that if we wanted to push price andperformance further (as customers requested), we’d have to develop our own chips, too. Our first generalizedchip was Graviton, which we announced in 2018. This helped a subset of customer workloads run morecost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and<br>innovating on a new chip, that we had something remarkable with our Graviton2 chip, which provides up to40% better price-performance than the comparable latest generation x86 processors. Think about howmuch of an impact 40% improvement on compute is. Compute is used for every bit of technology. That’s ahuge deal for customers. And, while Graviton2 has been a significant success thus far (48 of the top 50 AWSEC2 customers have already adopted it), the AWS Chips team was already learning from what customerssaid could be better, and announced Graviton3 this past December (offering a 25% improvement on top ofGraviton2’s relative gains). The list of what we’ve invented and delivered for customers in EC2 (and AWS ingeneral) is pretty mind-boggling, and this iterative approach to innovation has not only given customersmuch more functionality in AWS than they can find anywhere else (which is a significant differentiator), butalso allowed us to arrive at the much more game-changing offering that AWS is today.<br>Devices : Our first foray into devices was the Kindle, released in 2007. It was not the most sophisticated<br>industrial design (it was creamy white in color and the corners were uncomfortable for some people to hold),but revolutionary because it offered customers the ability to download any of over 90,000 books (nowmillions) in 60 seconds—and we got better and faster at building attractive designs. Shortly thereafter, welaunched a tablet, and then a phone (with the distinguishing feature of having front-facing cameras and agyroscope to give customers a dynamic perspective along with varied 3D experiences).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530577</td>\n",
       "      <td>Y ou saw this sort of balancingagain in 2008-2009 as we endured the recession provoked by the mortgage-backed securities financial crisis.We took several actions to manage the cost structure and efficiency of our Stores business, but we alsobalanced this streamlining with investment in customer experiences that we believed could be substantialfuture businesses with strong returns for shareholders. In 2008, AWS was still a fairly small, fledgling business.We knew we were on to something, but it still required substantial capital investment. There were voicesinside and outside of the company questioning why Amazon (known mostly as an online retailer then) wouldbe investing so much in cloud computing. But, we knew we were inventing something special that couldcreate a lot of value for customers and Amazon in the future. We had a head start on potential competitors;and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision tocontinue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, withstrong profitability, that has transformed how customers from start-ups to multinational companies to publicsector organizations manage their technology infrastructure. Amazon would be a different company ifwe’d slowed investment in AWS during that 2008-2009 period.<br>Change is always around the corner. Sometimes, you proactively invite it in, and sometimes it just comes<br>a-knocking. But, when you see it’s coming, you have to embrace it. And, the companies that do this well overa long period of time usually succeed. I’m optimistic about our future prospects because I like the way ourteam is responding to the changes we see in front of us.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(retrieved_nodes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad7d62b-dba9-45d5-896f-4baa9a40edba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retrieved_nodes1_withreranker \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieved_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow has AWS evolved?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_reranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mget_retrieved_nodes\u001b[0;34m(query_str, vector_top_k, reranker_top_n, with_reranker)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_reranker:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# configure reranker\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     reranker \u001b[38;5;241m=\u001b[39m LLMRerank(\n\u001b[1;32m     24\u001b[0m         choice_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m     25\u001b[0m         top_n\u001b[38;5;241m=\u001b[39mreranker_top_n, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrieved_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_nodes\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/postprocessor/types.py:48\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/postprocessor/llm_rerank.py:93\u001b[0m, in \u001b[0;36mLLMRerank._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# call each batch independently\u001b[39;00m\n\u001b[1;32m     87\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context\u001b[38;5;241m.\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice_select_prompt,\n\u001b[1;32m     89\u001b[0m     context_str\u001b[38;5;241m=\u001b[39mfmt_batch_str,\n\u001b[1;32m     90\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m raw_choices, relevances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_choice_select_answer_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m choice_idxs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(choice) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m raw_choices]\n\u001b[1;32m     97\u001b[0m choice_nodes \u001b[38;5;241m=\u001b[39m [nodes_batch[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m choice_idxs]\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/indices/utils.py:105\u001b[0m, in \u001b[0;36mdefault_parse_choice_select_answer_fn\u001b[0;34m(answer, num_choices, raise_error)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid answer line: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer line must be of the form: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_num: <int>, answer_relevance: <float>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         )\n\u001b[0;32m--> 105\u001b[0m answer_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mline_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer_num \u001b[38;5;241m>\u001b[39m num_choices:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieved_nodes1_withreranker = get_retrieved_nodes(\n",
    "    \"How has AWS evolved?\", \n",
    "    vector_top_k=20,\n",
    "    reranker_top_n=1,\n",
    "    with_reranker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d031c16-16ac-425e-ad56-3b9ef0dd0fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes1_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22d297-dd3b-4a71-a890-faceafab4e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_retrieved_nodes(retrieved_nodes1_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7033d487-3b31-4c95-9a63-afec75e3bc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes2 = get_retrieved_nodes(\n",
    "    \"Why is Amazon successful?\", vector_top_k=3, with_reranker=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "444a3658-740c-4efa-b73d-d102528ddf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "104633ac-d7ee-496d-8c4b-44aa462098f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.610493</td>\n",
       "      <td>That’s rarely, if ever, how it happens. One of the lesser known facts about innovativecompanies like Amazon is that they are relentlessly debating, re-defining, tinkering, iterating, andexperimenting to take the seed of a big idea and make it into something that resonates with customers andmeaningfully changes their customer experience over a long period of time.<br>Let me give you some Amazon examples.Our Fulfillment Network : Going back to the pandemic, there’s no way we could have started working on<br>our fulfillment network in March 2020 and satisfied anything close to what our customers needed. We’d beeninnovating in our fulfillment network for 20 years, constantly trying to shorten the time to get items to<br>customers. In the early 2000s, it took us an average of 18 hours to get an item through our fulfillment centersand on the right truck for shipment. Now, it takes us two. To deliver as reliably and cost-effectively as wedesire, and to serve Amazon Prime members expecting shipments in a couple of days, we spent years buildingout an expansive set of fulfillment centers, a substantial logistics and transportation capability, andreconfigured how we did virtually everything in our facilities. For perspective, in 2004, we had sevenfulfillment centers in the U.S. and four in other parts of the world, and we hadn’t yet added delivery stations,which connect our fulfillment and sortation centers to the last-mile delivery vans you see driving aroundyour neighborhood. Fast forward to the end of 2021, we had 253 fulfillment centers, 110 sortation centers,and 467 delivery stations in North America, with an additional 157 fulfillment centers, 58 sortation centers,and 588 delivery stations across the globe. Our delivery network grew to more than 260,000 driversworldwide, and our Amazon Air cargo fleet has more than 100 aircraft. This has represented a capitalinvestment of over $100 billion and countless iterations and small process improvements by over a millionAmazonians in the last decade and a half.<br>Ironically, just before COVID started, we’d made the decision to invest billions of incremental dollars over<br>several years to deliver an increasing number of Prime shipments in one day. This initiative was slowed by thechallenges of the pandemic, but we’ve since resumed our focus here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576479</td>\n",
       "      <td>How about employees? This is also a reasonably easy value creation question to answer because we can look<br>at compensation expense. What is an expense for a company is income for employees. In 2020, employees<br>earned $80 billion, plus another $11 billion to include benefits and various payroll taxes, for a total of<br>$91 billion.<br>How about third-party sellers? We have an internal team (the Selling Partner Services team) that works to<br>answer that question. They estimate that, in 2020, third-party seller profits from selling on Amazon were<br>between $25 billion and $39 billion, and to be conservative here I’ll go with $25 billion.<br>For customers, we have to break it down into consumer customers and AWS customers.<br>We’ll do consumers first. We offer low prices, vast selection, and fast delivery, but imagine we ignore all of<br>that for the purpose of this estimate and value only one thing: we save customers time.<br>Customers complete 28% of purchases on Amazon in three minutes or less, and half of all purchases are<br>finished in less than 15 minutes. Compare that to the typical shopping trip to a physical store – driving,<br>parking, searching store aisles, waiting in the checkout line, finding your car, and driving home. Research<br>suggests the typical physical store trip takes about an hour. If you assume that a typical Amazon purchase<br>takes 15 minutes and that it saves you a couple of trips to a physical store a week, that’s more than 75<br>hours a year saved. That’s important. We’re all busy in the early 21stcentury.<br>So that we can get a dollar figure, let’s value the time savings at $10 per hour, which is conservative. Seventy-<br>five hours multiplied by $10 an hour and subtracting the cost of Prime gives you value creation for each<br>Prime member of about $630. We have 200 million Prime members, for a total in 2020 of $126 billion of value<br>creation.<br>AWS is challenging to estimate because each customer’s workload is so different, but we’ll do it anyway,<br>acknowledging up front that the error bars are high. Direct cost improvements from operating in the cloud<br>versus on premises vary, but a reasonable estimate is 30%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.573374</td>\n",
       "      <td>through the pandemic the same way without the dedication and extraordinary efforts shown by our teams<br>during this period, and I’m eternally grateful.<br>It’s not normal for a company of any size to be able to respond to something as discontinuous and<br>unpredictable as this pandemic turned out to be. What is it about Amazon that made it possible for us to doso? It’s because we weren’t starting from a standing start. We had been iterating on and remaking ourfulfillment capabilities for nearly two decades. In every business we pursue, we’re constantly experimentingand inventing. We’re divinely discontented with customer experiences, whether they’re our own or not. Webelieve these customer experiences can always be better, and we strive to make customers’ lives better andeasier every day. The beauty of this mission is that you never run out of runway; customers always want better,and our job is both to listen to their feedback and to imagine what else is possible and invent on theirbehalf.<br>People often assume that the game-changing inventions they admire just pop out of somebody’s head, a<br>light bulb goes off, a team executes to that idea, and presto—you have a new invention that’s a breakawaysuccess for a long time. That’s rarely, if ever, how it happens. One of the lesser known facts about innovativecompanies like Amazon is that they are relentlessly debating, re-defining, tinkering, iterating, andexperimenting to take the seed of a big idea and make it into something that resonates with customers andmeaningfully changes their customer experience over a long period of time.<br>Let me give you some Amazon examples.Our Fulfillment Network : Going back to the pandemic, there’s no way we could have started working on<br>our fulfillment network in March 2020 and satisfied anything close to what our customers needed. We’d beeninnovating in our fulfillment network for 20 years, constantly trying to shorten the time to get items to<br>customers. In the early 2000s, it took us an average of 18 hours to get an item through our fulfillment centersand on the right truck for shipment. Now, it takes us two.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_retrieved_nodes(retrieved_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab0acde8-cb0a-4fa8-b30f-4fc965d8e7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retrieved_nodes2_withreranker \u001b[38;5;241m=\u001b[39m \u001b[43mget_retrieved_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhy is Amazon successful?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreranker_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_reranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mget_retrieved_nodes\u001b[0;34m(query_str, vector_top_k, reranker_top_n, with_reranker)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_reranker:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# configure reranker\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     reranker \u001b[38;5;241m=\u001b[39m LLMRerank(\n\u001b[1;32m     24\u001b[0m         choice_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m     25\u001b[0m         top_n\u001b[38;5;241m=\u001b[39mreranker_top_n, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrieved_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_nodes\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/postprocessor/types.py:48\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[0;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/postprocessor/llm_rerank.py:93\u001b[0m, in \u001b[0;36mLLMRerank._postprocess_nodes\u001b[0;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# call each batch independently\u001b[39;00m\n\u001b[1;32m     87\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_context\u001b[38;5;241m.\u001b[39mllm_predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice_select_prompt,\n\u001b[1;32m     89\u001b[0m     context_str\u001b[38;5;241m=\u001b[39mfmt_batch_str,\n\u001b[1;32m     90\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m raw_choices, relevances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_choice_select_answer_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m choice_idxs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(choice) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m raw_choices]\n\u001b[1;32m     97\u001b[0m choice_nodes \u001b[38;5;241m=\u001b[39m [nodes_batch[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m choice_idxs]\n",
      "File \u001b[0;32m~/Documents/tech/smkrlocal/env/lib/python3.11/site-packages/llama_index/indices/utils.py:105\u001b[0m, in \u001b[0;36mdefault_parse_choice_select_answer_fn\u001b[0;34m(answer, num_choices, raise_error)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid answer line: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_line\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer line must be of the form: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_num: <int>, answer_relevance: <float>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         )\n\u001b[0;32m--> 105\u001b[0m answer_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mline_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer_num \u001b[38;5;241m>\u001b[39m num_choices:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieved_nodes2_withreranker = get_retrieved_nodes(\n",
    "    \"Why is Amazon successful?\",\n",
    "    vector_top_k=3,\n",
    "    reranker_top_n=1,\n",
    "    with_reranker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba718e-4ae9-4e15-8739-4f68f2d046b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(retrieved_nodes2_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678820-d492-4c4a-b246-04a10e7d6b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_retrieved_nodes(retrieved_nodes2_withreranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ea771-7116-41e6-8032-89f4624cd6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.r5.4xlarge",
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
